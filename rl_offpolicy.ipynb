{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "69c362cb",
      "metadata": {
        "id": "69c362cb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import math \n",
        "from tqdm import tqdm "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Environment:\n",
        "    def __init__(self):\n",
        "        # encoded racetrack\n",
        "        self.env = [\n",
        "                        [0,0,0,0,0,0,0,0],\n",
        "                        [0,1,1,1,0,0,0,0],\n",
        "                        [0,1,1,1,1,1,1,1],\n",
        "                        [0,1,1,0,1,1,1,1],\n",
        "                        [0,1,1,0,1,0,1,1],\n",
        "                        [0,1,1,0,1,1,1,1],\n",
        "                        [0,1,1,1,1,0,1,1],\n",
        "                        [0,1,1,1,0,0,0,0],\n",
        "                        [0,1,1,1,0,0,0,0],\n",
        "                        [0,1,1,1,0,0,0,0],\n",
        "                        [0,1,1,1,0,0,0,0],\n",
        "                        [0,1,1,1,0,0,0,0],\n",
        "                        [0,1,1,1,1,0,0,0],\n",
        "                        [0,0,1,1,1,0,0,0],\n",
        "                        [0,0,1,1,1,0,0,0],\n",
        "                        [0,0,1,1,1,0,0,0],\n",
        "                        [0,0,1,1,1,0,0,0],\n",
        "                        [0,0,1,1,1,0,0,0],\n",
        "                        [0,0,1,1,1,0,0,0],\n",
        "                        [0,0,1,1,1,0,0,0],\n",
        "                        [0,0,0,0,0,0,0,0]\n",
        "                   ]\n",
        "\n",
        "        self.start = [(19,2),(19,3),(19,4)]\n",
        "        self.end = [(2,7),(3,7),(4,7),(5,7),(6,7)]\n",
        "        self.val = [\n",
        "        (1,1), (2,1), (3,1), (4,1), (5,1), (6,1), (7,1), (8,1), (9,1), (10,1), (11,1), (12,1),\n",
        "        (1,2), (2,2), (3,2), (4,2), (5,2), (6,2), (7,2), (8,2), (9,2), (10,2), (11,2), (12,2), (13,2), (14,2), (15,2), (16,2), (17,2), (18,2),\n",
        "        (1,3), (2,3), (6,3), (7,3), (8,3), (9,3), (10,3), (11,3), (12,3), (13,3), (14,3), (15,3), (16,3), (17,3), (18,3),\n",
        "        (2,4), (3,4), (4,4), (5,4), (6,4), (12,4), (13,4), (14,4), (15,4), (16,4), (17,4), (18,4),\n",
        "        (2,5), (3,5), (5,5),\n",
        "        (2,6), (3,6), (4,6), (5,6), (6,6)\n",
        "        ]\n",
        "        r,c = self.start[random.randrange(3)]\n",
        "        self.curr_state = (r,c,0,0)\n",
        "         \n",
        "\n",
        "    def reset_state(self):\n",
        "        x,y = self.start[random.randrange(3)]\n",
        "        self.curr_state = (x,y,0,0)\n",
        "        return self.curr_state\n",
        "\n",
        "\n",
        "    def check_within_track(self,x1,y1,x2,y2):\n",
        "        x = np.linspace(x2, x1, 11*(x1-x2))\n",
        "        if (x2 != x1):\n",
        "          y = y1 + ((y2-y1)/(x2-x1))*(x-x1)\n",
        "          path = []\n",
        "          for i in range(len(x)):\n",
        "            if not((round(x[i]), round(y[i])) in path):\n",
        "              path.append((round(x[i]), round(y[i])))\n",
        "          path.reverse() \n",
        "          \n",
        "          for i in path:\n",
        "              if not(i in self.val) and not(i in self.start) and not(i in self.end): \n",
        "                return 1\n",
        "              elif i in self.end:\n",
        "                return 2 \n",
        "          return 0\n",
        "\n",
        "        else:\n",
        "          y = np.linspace(y1, y2, (y2-y1)+1)\n",
        "          path = []\n",
        "          for i in range(len(y)):\n",
        "            if not((x1, round(y[i])) in path):\n",
        "              path.append((x1, round(y[i])))\n",
        "          path.reverse()\n",
        "          for i in path:\n",
        "              if not(i in self.val) and not(i in self.start) and not(i in self.end): \n",
        "                return 1\n",
        "              elif i in self.end:\n",
        "                return 2\n",
        "          return 0\n",
        "\n",
        "    def step(self,state,action):\n",
        "        x = state[0]\n",
        "        y = state[1]\n",
        "        vx_new = state[2] + action[0] \n",
        "        vy_new = state[3] + action[1]\n",
        "\n",
        "        if(vx_new > 5):\n",
        "            vx_new = 5\n",
        "        if(vx_new < 0):\n",
        "            vx_new = 0\n",
        "        if(vy_new > 5):\n",
        "            vy_new = 5\n",
        "        if(vy_new < 0):\n",
        "            vy_new = 0\n",
        "        \n",
        "        if(vx_new == 0 and vy_new == 0):\n",
        "            vx_new = 0\n",
        "            vy_new = 1\n",
        "        \n",
        "        x_new = x - vx_new\n",
        "        y_new = y + vy_new\n",
        "        \n",
        "        if(x_new < 0 or y_new > 7):\n",
        "            return (x_new,y_new,vx_new,vy_new), -100, True\n",
        "        \n",
        "        if(self.check_within_track(x,y,x_new,y_new) == 1):\n",
        "            return (x_new,y_new,vx_new,vy_new), -100, True\n",
        "\n",
        "        if(self.check_within_track(x,y,x_new,y_new) == 2):\n",
        "            return (x_new,y_new,vx_new,vy_new), 100, True\n",
        "        \n",
        "        if(self.check_within_track(x,y,x_new,y_new) == 0):\n",
        "            return (x_new,y_new,vx_new,vy_new), -1, False\n",
        "            "
      ],
      "metadata": {
        "id": "w72kcJBeJcuH"
      },
      "id": "w72kcJBeJcuH",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MonteCarlo_Offpolicy:\n",
        "    def __init__(self):\n",
        "        self.qsa = dict()\n",
        "        self.c = dict()\n",
        "        self.b = dict()\n",
        "        self.policy = dict()\n",
        "        self.action = []\n",
        "        self.reward = []\n",
        "        self.envi = Environment()\n",
        "        l = [-1,0,1]\n",
        "        for i in l:\n",
        "            for j in l:\n",
        "                self.action.append((i,j))\n",
        "\n",
        "        for r in range(21):\n",
        "            for c in range(8):\n",
        "                for vx in range(6):\n",
        "                    for vy in range(6):\n",
        "                        ac = self.action[random.randrange(0,9)]\n",
        "                        for act in self.action:\n",
        "                            if act == ac:\n",
        "                              self.policy[((r,c,vx,vy), act)] = 1\n",
        "                            else:\n",
        "                              self.policy[((r,c,vx,vy), act)] = 0\n",
        "                            self.qsa[((r,c,vx,vy), act)] = random.randint(-123,100)\n",
        "                            self.c[((r,c,vx,vy), act)] = 0\n",
        "                            self.b[((r,c,vx,vy), act)] = 1/9\n",
        "                            \n",
        "\n",
        "\n",
        "    def generate_episode(self):\n",
        "        episode = []\n",
        "        state = self.envi.reset_state()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            l = []\n",
        "            for act in self.action:\n",
        "                l.append(self.b[(state,act)])\n",
        "\n",
        "            action = random.choices(self.action, weights=l, k=1)[0]\n",
        "            next_state, reward, done = self.envi.step(state,action)\n",
        "            episode.append([state, action, reward, next_state])\n",
        "            state = next_state\n",
        "            if state[0] >= 0 and state[0] <= 20 and state[1] < 8:\n",
        "              if self.envi.env[state[0]][state[1]] == 0:\n",
        "                break\n",
        "        return episode\n",
        "\n",
        "\n",
        "    \n",
        "    def algo(self):\n",
        "        rewards = []\n",
        "        temp_reward = 0\n",
        "        itr = 300000\n",
        "        for i in tqdm(range(itr)):\n",
        "            episode = self.generate_episode()\n",
        "            # print(episode)\n",
        "            g=0\n",
        "            w=1\n",
        "            for i in range(len(episode)-1,-1,-1):\n",
        "                g =  episode[i][2]+0.47*g\n",
        "                self.c[(episode[i][0],episode[i][1])]+=w                  \n",
        "                self.qsa[(episode[i][0],episode[i][1])] = self.qsa[(episode[i][0],episode[i][1])] + (w/self.c[(episode[i][0],episode[i][1])]) * (g - self.qsa[(episode[i][0],episode[i][1])])\n",
        "                # s=episode[i][0]\n",
        "                # a_max = self.argmax(self.qsa,s)\n",
        "\n",
        "                q = []\n",
        "                for action in self.action:\n",
        "                    q.append(self.qsa[(episode[i][0],action)])\n",
        "                \n",
        "                a_star = np.argmax(q)\n",
        "        \n",
        "            \n",
        "                for j in range(0,len(self.action)):\n",
        "                    if j==a_star:\n",
        "                        self.b[(episode[i][0],self.action[j])] = 0.8 + 0.2/9\n",
        "                        self.policy[(episode[i][0],self.action[j])] = 1\n",
        "                    \n",
        "                    else:\n",
        "                        self.b[(episode[i][0],self.action[j])] = 0.2/9\n",
        "                        self.policy[(episode[i][0],self.action[j])] = 0\n",
        "                \n",
        "                if episode[i][1]==self.action[a_star]:\n",
        "                    w = w/self.b[(episode[i][0],episode[i][1])]\n",
        "                \n",
        "                else:\n",
        "                    break\n",
        "\n",
        "            if episode[-1][2] == -100:\n",
        "              temp_reward += len(episode)\n",
        "            else:\n",
        "              temp_reward += len(episode) - 1\n",
        "              rewards.append(temp_reward)\n",
        "              temp_reward = 0\n",
        "        self.reward = rewards\n"
      ],
      "metadata": {
        "id": "pSwBAWigGd5N"
      },
      "id": "pSwBAWigGd5N",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "3f5208dc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f5208dc",
        "outputId": "a193854e-c5da-4376-b08f-f018a3c1ffe9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300000/300000 [15:08<00:00, 330.17it/s]\n"
          ]
        }
      ],
      "source": [
        "policy_obj = MonteCarlo_Offpolicy()\n",
        "policy_obj.algo() "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = Environment()\n",
        "episode = []\n",
        "start = (19,2,0,0)\n",
        "done = False\n",
        "prev_state = start\n",
        "curr_state = start\n",
        "\n",
        "while not done:\n",
        "            # po = dict()\n",
        "            print(curr_state,\"\\n\")\n",
        "            qsa = []\n",
        "            for act in policy_obj.action:\n",
        "                qsa.append(policy_obj.qsa[(prev_state,act)])\n",
        "\n",
        "            a_star = np.argmax(qsa)\n",
        "            action = policy_obj.action[a_star]\n",
        "            curr_state, reward, done = env.step(prev_state,action)\n",
        "            # valid = self.envi.check_within_track(state[0],state[1],next_state[0],next_state[1])\n",
        "            episode.append([prev_state, action, reward, curr_state])\n",
        "            prev_state = curr_state\n",
        "            # if prev_state[0] >= 0 and prev_state[0] <= 20 and prev_state[1] < 8:\n",
        "            #   if env.env[prev_state[0]][prev_state[1]] == 0:\n",
        "            #     break\n",
        "\n",
        "episode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kT3tQTZuaUZV",
        "outputId": "ebefdae1-f60b-435a-aac3-66bf82781fdc"
      },
      "id": "kT3tQTZuaUZV",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(19, 2, 0, 0) \n",
            "\n",
            "(18, 2, 1, 0) \n",
            "\n",
            "(16, 2, 2, 0) \n",
            "\n",
            "(13, 2, 3, 0) \n",
            "\n",
            "(11, 2, 2, 0) \n",
            "\n",
            "(9, 2, 2, 0) \n",
            "\n",
            "(7, 3, 2, 1) \n",
            "\n",
            "(5, 5, 2, 2) \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[(19, 2, 0, 0), (1, 0), -1, (18, 2, 1, 0)],\n",
              " [(18, 2, 1, 0), (1, -1), -1, (16, 2, 2, 0)],\n",
              " [(16, 2, 2, 0), (1, -1), -1, (13, 2, 3, 0)],\n",
              " [(13, 2, 3, 0), (-1, -1), -1, (11, 2, 2, 0)],\n",
              " [(11, 2, 2, 0), (0, 0), -1, (9, 2, 2, 0)],\n",
              " [(9, 2, 2, 0), (0, 1), -1, (7, 3, 2, 1)],\n",
              " [(7, 3, 2, 1), (0, 1), -1, (5, 5, 2, 2)],\n",
              " [(5, 5, 2, 2), (-1, 0), 100, (4, 7, 1, 2)]]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(np.array(policy_obj.reward))"
      ],
      "metadata": {
        "id": "Peur5MqLtbdR"
      },
      "id": "Peur5MqLtbdR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b8ccc56",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6b8ccc56",
        "outputId": "ae65b4f3-acf4-4691-f790-d1a76cb58c51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(19, 3, 0, 0) \n",
            "\n",
            "(18, 3, 1, 0) \n",
            "\n",
            "(16, 3, 2, 0) \n",
            "\n",
            "(13, 3, 3, 0) \n",
            "\n",
            "(10, 3, 3, 0) \n",
            "\n",
            "(7, 3, 3, 0) \n",
            "\n",
            "(4, 4, 3, 1) \n",
            "\n",
            "(2, 6, 2, 2) \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[((19, 3, 0, 0), (1, 0), -1, (18, 3, 1, 0)),\n",
              " ((18, 3, 1, 0), (1, -1), -1, (16, 3, 2, 0)),\n",
              " ((16, 3, 2, 0), (1, 0), -1, (13, 3, 3, 0)),\n",
              " ((13, 3, 3, 0), (0, -1), -1, (10, 3, 3, 0)),\n",
              " ((10, 3, 3, 0), (0, -1), -1, (7, 3, 3, 0)),\n",
              " ((7, 3, 3, 0), (0, 1), -1, (4, 4, 3, 1)),\n",
              " ((4, 4, 3, 1), (-1, 1), -1, (2, 6, 2, 2)),\n",
              " ((2, 6, 2, 2), (-1, 0), 100, (2, 7))]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "env = Environment()\n",
        "episode = []\n",
        "start = (19, 3, 0, 0)\n",
        "done = False\n",
        "prev_state = start\n",
        "curr_state = start\n",
        "\n",
        "while not done:\n",
        "    print(curr_state,\"\\n\")\n",
        "    qsa = []\n",
        "    for action in policy_obj.actions:\n",
        "        qsa.append(policy_obj.q_table[(prev_state,action)])\n",
        "    a_star = np.argmax(qsa)\n",
        "    action = policy_obj.actions[a_star]\n",
        "    curr_state = env.next_state(prev_state,action)\n",
        "    episode_end, curr_state = env.check_red(prev_state, curr_state)\n",
        "    reward = env.reward(curr_state)\n",
        "    episode.append((prev_state,action,reward,curr_state))\n",
        "    prev_state = curr_state\n",
        "\n",
        "episode\n",
        "    \n",
        "        "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
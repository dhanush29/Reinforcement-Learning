{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Yu9Xyx-uJ3HQ"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import math\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "uEPyWJCXJ3HR"
      },
      "outputs": [],
      "source": [
        "class Environment:\n",
        "    def __init__(self):\n",
        "        # encoded racetrack\n",
        "        self.env = [\n",
        "                        [0,0,0,0,0,0,0,0],\n",
        "                        [0,1,1,1,0,0,0,0],\n",
        "                        [0,1,1,1,1,1,1,1],\n",
        "                        [0,1,1,0,1,1,1,1],\n",
        "                        [0,1,1,0,1,0,1,1],\n",
        "                        [0,1,1,0,1,1,1,1],\n",
        "                        [0,1,1,1,1,0,1,1],\n",
        "                        [0,1,1,1,0,0,0,0],\n",
        "                        [0,1,1,1,0,0,0,0],\n",
        "                        [0,1,1,1,0,0,0,0],\n",
        "                        [0,1,1,1,0,0,0,0],\n",
        "                        [0,1,1,1,0,0,0,0],\n",
        "                        [0,1,1,1,1,0,0,0],\n",
        "                        [0,0,1,1,1,0,0,0],\n",
        "                        [0,0,1,1,1,0,0,0],\n",
        "                        [0,0,1,1,1,0,0,0],\n",
        "                        [0,0,1,1,1,0,0,0],\n",
        "                        [0,0,1,1,1,0,0,0],\n",
        "                        [0,0,1,1,1,0,0,0],\n",
        "                        [0,0,1,1,1,0,0,0],\n",
        "                        [0,0,0,0,0,0,0,0]\n",
        "                   ]\n",
        "\n",
        "        self.start = [(19,2),(19,3),(19,4)]\n",
        "        self.end = [(2,7),(3,7),(4,7),(5,7),(6,7)]\n",
        "        self.val = [\n",
        "        (1,1), (2,1), (3,1), (4,1), (5,1), (6,1), (7,1), (8,1), (9,1), (10,1), (11,1), (12,1),\n",
        "        (1,2), (2,2), (3,2), (4,2), (5,2), (6,2), (7,2), (8,2), (9,2), (10,2), (11,2), (12,2), (13,2), (14,2), (15,2), (16,2), (17,2), (18,2),\n",
        "        (1,3), (2,3), (6,3), (7,3), (8,3), (9,3), (10,3), (11,3), (12,3), (13,3), (14,3), (15,3), (16,3), (17,3), (18,3),\n",
        "        (2,4), (3,4), (4,4), (5,4), (6,4), (12,4), (13,4), (14,4), (15,4), (16,4), (17,4), (18,4),\n",
        "        (2,5), (3,5), (5,5),\n",
        "        (2,6), (3,6), (4,6), (5,6), (6,6)\n",
        "        ]\n",
        "        r,c = self.start[random.randrange(3)]\n",
        "        self.curr_state = (r,c,0,0)\n",
        "         \n",
        "\n",
        "    def reset_state(self):\n",
        "        x,y = self.start[random.randrange(3)]\n",
        "        self.curr_state = (x,y,0,0)\n",
        "        return self.curr_state\n",
        "\n",
        "\n",
        "    def check_within_track(self,x1,y1,x2,y2):\n",
        "        x = np.linspace(x2, x1, 11*(x1-x2))\n",
        "        if (x2 != x1):\n",
        "          y = y1 + ((y2-y1)/(x2-x1))*(x-x1)\n",
        "          path = []\n",
        "          for i in range(len(x)):\n",
        "            if not((round(x[i]), round(y[i])) in path):\n",
        "              path.append((round(x[i]), round(y[i])))\n",
        "          path.reverse() \n",
        "          \n",
        "          for i in path:\n",
        "              if not(i in self.val) and not(i in self.start) and not(i in self.end): \n",
        "                return 1\n",
        "              elif i in self.end:\n",
        "                return 2 \n",
        "          return 0\n",
        "\n",
        "        else:\n",
        "          y = np.linspace(y1, y2, (y2-y1)+1)\n",
        "          path = []\n",
        "          for i in range(len(y)):\n",
        "            if not((x1, round(y[i])) in path):\n",
        "              path.append((x1, round(y[i])))\n",
        "          path.reverse()\n",
        "          for i in path:\n",
        "              if not(i in self.val) and not(i in self.start) and not(i in self.end): \n",
        "                return 1\n",
        "              elif i in self.end:\n",
        "                return 2\n",
        "          return 0\n",
        "\n",
        "    def step(self,state,action):\n",
        "        x = state[0]\n",
        "        y = state[1]\n",
        "        vx_new = state[2] + action[0] \n",
        "        vy_new = state[3] + action[1]\n",
        "\n",
        "        if(vx_new > 5):\n",
        "            vx_new = 5\n",
        "        if(vx_new < 0):\n",
        "            vx_new = 0\n",
        "        if(vy_new > 5):\n",
        "            vy_new = 5\n",
        "        if(vy_new < 0):\n",
        "            vy_new = 0\n",
        "        \n",
        "        if(vx_new == 0 and vy_new == 0):\n",
        "            vx_new = 0\n",
        "            vy_new = 1\n",
        "        \n",
        "        x_new = x - vx_new\n",
        "        y_new = y + vy_new\n",
        "        \n",
        "        if(x_new < 0 or y_new > 7):\n",
        "            return (x_new,y_new,vx_new,vy_new), -100, True\n",
        "        \n",
        "        if(self.check_within_track(x,y,x_new,y_new) == 1):\n",
        "            return (x_new,y_new,vx_new,vy_new), -100, True\n",
        "\n",
        "        if(self.check_within_track(x,y,x_new,y_new) == 2):\n",
        "            return (x_new,y_new,vx_new,vy_new), 100, True\n",
        "        \n",
        "        if(self.check_within_track(x,y,x_new,y_new) == 0):\n",
        "            return (x_new,y_new,vx_new,vy_new), -1, False\n",
        "        # else:\n",
        "        #     if (x_new,y_new) in self.end:\n",
        "        #         return (x_new,y_new,vx_new,vy_new), 100, True\n",
        "        #     else:\n",
        "        #         return (x_new,y_new,vx_new,vy_new), -1, False\n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80jc4pbLEwu_",
        "outputId": "345b6d37-5590-4fe7-8fe6-8d06ca0f6599"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "e = Environment()\n",
        "# e.step((7,3,0,0),(3,5))\n",
        "e.check_within_track(7,3,4,4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "H3SdT_CSJ3HV"
      },
      "outputs": [],
      "source": [
        "class MonteCarlo_Onpolicy:\n",
        "    def __init__(self):\n",
        "        self.qsa = dict()\n",
        "        self.reward = dict()\n",
        "        self.policy = dict()\n",
        "        self.action = []\n",
        "        self.envi = Environment()\n",
        "        l = [-1,0,1]\n",
        "        for i in l:\n",
        "            for j in l:\n",
        "                self.action.append((i,j))\n",
        "\n",
        "        for r in range(21):\n",
        "            for c in range(8):\n",
        "                for vx in range(6):\n",
        "                    for vy in range(6):\n",
        "                        for act in self.action:\n",
        "                            self.qsa[((r,c,vx,vy), act)] = random.random()\n",
        "                            self.reward[((r,c,vx,vy), act)] = []\n",
        "                            self.policy[((r,c,vx,vy), act)] = 1/9\n",
        "\n",
        "\n",
        "    def generate_episode(self):\n",
        "        episode = []\n",
        "        state = self.envi.reset_state()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            l = []\n",
        "            for act in self.action:\n",
        "                l.append(self.policy[(state,act)])\n",
        "            action = random.choices(self.action, weights=l, k=1)[0]\n",
        "            next_state, reward, done = self.envi.step(state,action)\n",
        "            episode.append([state, action, reward, next_state])\n",
        "            state = next_state\n",
        "            if state[0] >= 0 and state[0] <= 20 and state[1] < 8:\n",
        "              if self.envi.env[state[0]][state[1]] == 0:\n",
        "                break\n",
        "            else:\n",
        "              break\n",
        "        return episode\n",
        "\n",
        "\n",
        "    def algo(self):\n",
        "        itr = 300000\n",
        "        for i in tqdm(range(itr)):\n",
        "            episode = self.generate_episode()\n",
        "            # print(episode)\n",
        "            sum=0\n",
        "            g=0\n",
        "            for i in range(len(episode)-1,-1,-1):\n",
        "                g = 0.47*g + episode[i][2]\n",
        "                self.reward[(episode[i][0],episode[i][1])].append(g)\n",
        "                s=0\n",
        "                for j in self.reward[(episode[i][0],episode[i][1])]:\n",
        "                    s+=j                    \n",
        "                self.qsa[(episode[i][0],episode[i][1])] = s/len(self.reward[(episode[i][0],episode[i][1])])\n",
        "            \n",
        "            q = []\n",
        "            for action in self.action:       \n",
        "               q.append(self.qsa[(episode[i][0],action)])\n",
        "                \n",
        "            a_star = np.argmax(q)\n",
        "            \n",
        "            for j in range(0,len(self.action)):\n",
        "                if j==a_star:\n",
        "                    self.policy[(episode[i][0],self.action[j])] = 0.8 + 0.2/9\n",
        "                    \n",
        "                else:\n",
        "                    self.policy[(episode[i][0],self.action[j])] = 0.2/9"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "policy_obj = MonteCarlo_Onpolicy()"
      ],
      "metadata": {
        "id": "rkjiKehJjB3J"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "policy_obj.algo()"
      ],
      "metadata": {
        "id": "bYbWKpNewoff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84b0ac05-9019-426a-ab45-37bb5fb7cc65"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300000/300000 [37:48<00:00, 132.26it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(np.array(policy_obj.reward))"
      ],
      "metadata": {
        "id": "ExhMShRI5kl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = Environment()\n",
        "episode = []\n",
        "start = (19,2,0,0)\n",
        "done = False\n",
        "prev_state = start\n",
        "curr_state = start\n",
        "\n",
        "while not done:\n",
        "            # po = dict()\n",
        "            qsa = []\n",
        "            for act in policy_obj.action:\n",
        "                qsa.append(policy_obj.qsa[(prev_state,act)])\n",
        "\n",
        "            a_star = np.argmax(qsa)\n",
        "            action = policy_obj.action[a_star]\n",
        "            curr_state, reward, done = env.step(prev_state,action)\n",
        "            episode.append([prev_state, action, reward, curr_state])\n",
        "            prev_state = curr_state\n",
        "\n",
        "episode"
      ],
      "metadata": {
        "id": "BQO1Zej_gNEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = Environment()\n",
        "episode = []\n",
        "start = (19,3,0,0)\n",
        "done = False\n",
        "prev_state = start\n",
        "curr_state = start\n",
        "\n",
        "while not done:\n",
        "            # po = dict()\n",
        "            qsa = []\n",
        "            for act in policy_obj.action:\n",
        "                qsa.append(policy_obj.qsa[(prev_state,act)])\n",
        "\n",
        "            a_star = np.argmax(qsa)\n",
        "            action = policy_obj.action[a_star]\n",
        "            curr_state, reward, done = env.step(prev_state,action)\n",
        "            episode.append([prev_state, action, reward, curr_state])\n",
        "            prev_state = curr_state\n",
        "\n",
        "episode"
      ],
      "metadata": {
        "id": "BWMvpNKSjORP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "854ad7b50770bedaf0cab730b1aaabb765566ea98036f134b639e260bede141e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}